metaMonitoring:
  prometheusRule:
    enabled: false
    mimirAlerts: true
    mimirRules: true
    groups:
    - name: mimir_api_1
      rules:
      - expr: histogram_quantile(0.99, sum(rate(cortex_request_duration_seconds_bucket[1m]))
          by (le, cluster, job))
        record: cluster_job:cortex_request_duration_seconds:99quantile
      - expr: histogram_quantile(0.50, sum(rate(cortex_request_duration_seconds_bucket[1m]))
          by (le, cluster, job))
        record: cluster_job:cortex_request_duration_seconds:50quantile
      - expr: sum(rate(cortex_request_duration_seconds_sum[1m])) by (cluster, job) / sum(rate(cortex_request_duration_seconds_count[1m]))
          by (cluster, job)
        record: cluster_job:cortex_request_duration_seconds:avg
      - expr: sum(rate(cortex_request_duration_seconds_bucket[1m])) by (le, cluster, job)
        record: cluster_job:cortex_request_duration_seconds_bucket:sum_rate
      - expr: sum(rate(cortex_request_duration_seconds_sum[1m])) by (cluster, job)
        record: cluster_job:cortex_request_duration_seconds_sum:sum_rate
      - expr: sum(rate(cortex_request_duration_seconds_count[1m])) by (cluster, job)
        record: cluster_job:cortex_request_duration_seconds_count:sum_rate
  serviceMonitor:
    enabled: false
  grafanaAgent:
    enabled: false
    installOperator: true
    metrics:
      enabled: true
      additionalRemoteWriteConfigs:
        - url: "http://grafana-mimir-nginx.monitoring.svc:80/api/v1/push"

serviceAccount:
  # -- Whether to create a service account or not. In case 'create' is false, do set 'name' to an existing service account name.
  create: true
  # -- Override for the generated service account name.
  name: grafana-mimir-sa
  annotations:
    eks.amazonaws.com/role-arn: "${s3_role_arn}"

minio:
  enabled: false

mimir:
  config: |
    usage_stats:
      installation_mode: helm

    activity_tracker:
      filepath: /active-query-tracker/activity.log

    {{- if .Values.enterprise.enabled }}
    admin_api:
      leader_election:
        enabled: true
        ring:
          kvstore:
            store: "memberlist"

    admin_client:
      storage:
      {{- if .Values.minio.enabled }}
        type: s3
        s3:
          access_key_id: {{ .Values.minio.rootUser }}
          bucket_name: enterprise-metrics-admin
          endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
          insecure: true
          secret_access_key: {{ .Values.minio.rootPassword }}
      {{- end }}
      {{- if (index .Values "admin-cache" "enabled") }}
        cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.adminCacheAddress" . }}
            max_item_size: {{ mul (index .Values "admin-cache").maxItemMemory 1024 1024 }}
      {{- end }}
    {{- end }}

    alertmanager:
      data_dir: /data
      enable_api: true
      external_url: /alertmanager
      {{- if .Values.alertmanager.zoneAwareReplication.enabled }}
      sharding_ring:
        zone_awareness_enabled: true
      {{- end }}
      {{- if .Values.alertmanager.fallbackConfig }}
      fallback_config_file: /configs/alertmanager_fallback_config.yaml
      {{- end }}

    {{- if .Values.minio.enabled }}
    alertmanager_storage:
      backend: s3
      s3:
        access_key_id: {{ .Values.minio.rootUser }}
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
        insecure: true
        secret_access_key: {{ .Values.minio.rootPassword }}
    {{- end }}

    {{- if .Values.enterprise.enabled }}
    auth:
      type: enterprise
    {{- end }}

    # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.
    blocks_storage:
      backend: s3
      bucket_store:
        {{- if index .Values "chunks-cache" "enabled" }}
        chunks_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.chunksCacheAddress" . }}
            max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
            timeout: 750ms
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "index-cache" "enabled" }}
        index_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.indexCacheAddress" . }}
            max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
            timeout: 750ms
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "metadata-cache" "enabled" }}
        metadata_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.metadataCacheAddress" . }}
            max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
            max_idle_connections: 150
        {{- end }}
        sync_dir: /data/tsdb-sync
      {{- if .Values.minio.enabled }}
      s3:
        access_key_id: {{ .Values.minio.rootUser }}
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-tsdb
        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
        insecure: true
        secret_access_key: {{ .Values.minio.rootPassword }}
      {{- end }}
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 3
        wal_compression_enabled: true

    {{- if .Values.enterprise.enabled }}
    cluster_name: "{{ .Release.Name }}"
    {{- end }}

    compactor:
      compaction_interval: 30m
      deletion_delay: 2h
      max_closing_blocks_concurrency: 2
      max_opening_blocks_concurrency: 4
      symbols_flushers_concurrency: 4
      first_level_compaction_wait_period: 25m
      data_dir: "/data"
      sharding_ring:
        wait_stability_min_duration: 1m
        heartbeat_period: 1m
        heartbeat_timeout: 4m

    distributor:
      ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m

    frontend:
      parallelize_shardable_queries: true
      {{- if index .Values "results-cache" "enabled" }}
      results_cache:
        backend: memcached
        memcached:
          timeout: 500ms
          addresses: {{ include "mimir.resultsCacheAddress" . }}
          max_item_size: {{ mul (index .Values "results-cache").maxItemMemory 1024 1024 }}
      cache_results: true
      query_sharding_target_series_per_shard: 2500
      {{- end }}
      {{- if .Values.query_scheduler.enabled }}
      scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- end }}

    frontend_worker:
      grpc_client_config:
        max_send_msg_size: 419430400 # 400MiB
      {{- if .Values.query_scheduler.enabled }}
      scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- else }}
      frontend_address: {{ template "mimir.fullname" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- end }}

    {{- if and .Values.enterprise.enabled }}
    gateway:
      proxy:
        admin_api:
          url: http://{{ template "mimir.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        alertmanager:
          url: http://{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        compactor:
          url: http://{{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        default:
          url: http://{{ template "mimir.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        distributor:
          url: dns:///{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverGrpcListenPort" . }}
        ingester:
          url: http://{{ template "mimir.fullname" . }}-ingester-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        query_frontend:
          url: http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        ruler:
          url: http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        store_gateway:
          url: http://{{ template "mimir.fullname" . }}-store-gateway-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}
        graphite_write_proxy:
          url: http://{{ template "mimir.fullname" . }}-graphite-write-proxy.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        graphite_querier:
          url: http://{{ template "mimir.fullname" . }}-graphite-querier.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
        {{- end}}
    {{- end }}

    ingester:
      ring:
        final_sleep: 0s
        num_tokens: 512
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        heartbeat_period: 2m
        heartbeat_timeout: 10m
        {{- if .Values.ingester.zoneAwareReplication.enabled }}
        zone_awareness_enabled: true
        {{- end }}

    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600

    {{- if .Values.enterprise.enabled }}
    instrumentation:
      enabled: true
      distributor_client:
        address: dns:///{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverGrpcListenPort" . }}

    license:
      path: "/license/license.jwt"
    {{- end }}

    limits:
      # Limit queries to 500 days. You can override this on a per-tenant basis.
      max_total_query_length: 12000h
      # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
      # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
      max_query_parallelism: 240
      # Avoid caching results newer than 10m because some samples can be delayed
      # This presents caching incomplete results
      max_cache_freshness: 10m

    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      join_members:
      - dns+{{ include "mimir.fullname" . }}-gossip-ring.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.memberlistBindPort" . }}

    querier:
      # With query sharding we run more but smaller queries. We must strike a balance
      # which allows us to process more sharded queries in parallel when requested, but not overload
      # queriers during non-sharded queries.
      max_concurrent: 16

    query_scheduler:
      # Increase from default of 100 to account for queries created by query sharding
      max_outstanding_requests_per_tenant: 800

    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
      enable_api: true
      rule_path: /data
      {{- if .Values.ruler.remoteEvaluationDedicatedQueryPath }}
      query_frontend:
        address: dns:///{{ template "mimir.fullname" . }}-ruler-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" .  }}
      {{- end }}

    {{- if or (.Values.minio.enabled) (index .Values "metadata-cache" "enabled") }}
    ruler_storage:
      {{- if .Values.minio.enabled }}
      backend: s3
      s3:
        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
        access_key_id: {{ .Values.minio.rootUser }}
        secret_access_key: {{ .Values.minio.rootPassword }}
        insecure: true
      {{- end }}
      {{- if index .Values "metadata-cache" "enabled" }}
      cache:
        backend: memcached
        memcached:
          addresses: {{ include "mimir.metadataCacheAddress" . }}
          max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
      {{- end }}
    {{- end }}

    runtime_config:
      file: /var/{{ include "mimir.name" . }}/runtime.yaml

    store_gateway:
      sharding_ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
        wait_stability_min_duration: 1m
        {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
        kvstore:
          prefix: multi-zone/
        {{- end }}
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
        zone_awareness_enabled: true
        {{- end }}

    {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}
    graphite:
      enabled: true

      write_proxy:
        distributor_client:
          address: dns:///{{ template "mimir.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" .  }}

      querier:
        remote_read:
          query_address: http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" .  }}/prometheus
        proxy_bad_requests: false

        schemas:
          default_storage_schemas_file: /etc/graphite-proxy/storage-schemas.conf
          default_storage_aggregations_file: /etc/graphite-proxy/storage-aggregations.conf
        aggregation_cache:
          memcached:
            addresses: dnssrvnoa+{{ template "mimir.fullname" . }}-gr-aggr-cache.{{ .Release.Namespace}}.svc:11211
            timeout: 1s
        metric_name_cache:
          memcached:
            addresses: dnssrvnoa+{{ template "mimir.fullname" . }}-gr-metricname-cache.{{ .Release.Namespace}}.svc:11211
            timeout: 1s
    {{- end}}

  structuredConfig:
    limits:
      out_of_order_time_window: 1h
    blocks_storage:
      backend: s3
      s3:
        bucket_name: ${s3_bucket_name}
        endpoint: s3.${s3_bucket_region}.amazonaws.com
        region: ${s3_bucket_region}
        sse:
          type: "SSE-S3"
      tsdb:
        retention_period: 48h

alertmanager:
  enabled: false
  persistentVolume:
    storageClass: ""
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

ruler:
  enabled: false
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

distributor:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"
  extraArgs:
    distributor.ingestion-rate-limit: "10000000000000"
  resources:
    requests:
      cpu: 100m
      memory: 512Mi

ingester:
  replicas: 3
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
  persistentVolume:
    storageClass: ${storage_class_name}
    size: 2Gi
    enableRetentionPolicy: false
    whenDeleted: Retain
    whenScaled: Retain
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      app.kubernetes.io/component: ingester
  extraArgs:
    ingester.max-global-series-per-user: "0"
    ingester.max-global-series-per-metric: "0"

overrides_exporter:
  enabled: false
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

querier:
  replicas: 2
  kedaAutoscaling:
    enabled: false
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
  extraArgs:
    querier.max-fetched-chunks-per-query: "8000000"

query_frontend:
  replicas: 1
  kedaAutoscaling:
    enabled: false
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
  affinity:
    nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: "Addons-Services"
              operator: In
              values:
              - "true"

query_scheduler:
  enabled: false
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

store_gateway:
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 512M
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      app.kubernetes.io/component: store-gateway
  persistentVolume:
    storageClass: ${storage_class_name}
    size: 2Gi
    enableRetentionPolicy: false
    whenDeleted: Retain
    whenScaled: Retain

compactor:
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      app.kubernetes.io/component: compactor
  persistentVolume:
    storageClass: ${storage_class_name}
    size: 2Gi
    enableRetentionPolicy: false
    whenDeleted: Retain
    whenScaled: Retain
